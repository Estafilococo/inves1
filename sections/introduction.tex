%%%
 % File: /latex/big-cocluster-paper/introduction.tex
 % Created Date: Tuesday, December 26th, 2023
 % Author: Zihan
 % -----
 % Last Modified: Friday, 9th February 2024 8:06:15 pm
 % Modified By: the developer formerly known as Zihan at <wzh4464@gmail.com>
 % -----
 % HISTORY:
 % Date      		By   	Comments
 % ----------		------	---------------------------------------------------------
%%%

\section{Introduction}
Artificial Intelligence (AI) has rapidly evolved into a pivotal technology, reshaping diverse domains from healthcare to finance. Its widespread impact is largely attributed to its ability to analyze complex datasets, uncover patterns, and facilitate decision-making processes. A critical function within AI is the domain of clustering, an unsupervised learning technique pivotal for data categorization and pattern recognition. Clustering algorithms, such as k-means\cite{lloyd1982LeastSquaresQuantization, macqueen1967MethodsClassificationAnalysis} and Gaussian Mixture Model (GMM)\cite{dempster1977MaximumLikelihoodIncomplete}, serve to aggregate data points based on shared attributes, thereby simplifying and interpreting the underlying data structure. However, these clustering algorithms often grapple with limitations, particularly when handling large, high-dimensional datasets. Their tendency to consider all features of data objects uniformly often leads to oversimplified interpretations and overlooks more nuanced, context-specific relationships within the data\cite{chen2023FastFlexibleBipartite, zhao2023MultiviewCoclusteringMultisimilarity, kumar2023CoclusteringBasedMethods}.

\textit{Co-clustering}\cite{cheng2000BiclusteringExpressionData, kluger2003SpectralBiclusteringMicroarray, yan2017CoclusteringMultidimensionalBig} is an approach that addresses these shortcomings by simultaneously grouping both rows (objects) and columns (features), revealing intricate correlations between two distinct data types, unlike other traditional cluster methods\cite{zhang2023AdaptiveGraphConvolution, yuan2023JointNetworkTopology, wu2023EffectiveClusteringStructured} that cluster either rows (objects) or columns (features) in isolation. This approach is particularly transformative in scenarios where the relationships between rows and columns are as crucial as the individual entities themselves. For example, in bioinformatics, co-clustering helps in identifying gene patterns by analyzing genes and conditions concurrently, thereby revealing more comprehensive biological insights\cite{higham2007SpectralClusteringIts, kluger2003SpectralBiclusteringMicroarray, madeira2004BiclusteringAlgorithmsBiological, zhao2012BiclusteringAnalysisPattern, golchev2015BiclusteringAnalysisGene}. Similarly, in text mining and recommender systems, it enables the discovery of latent relationships between users and items or documents and terms\cite{busygin2008BiclusteringDataMining, dhillon2001CoclusteringDocumentsWords, dhillon2007WeightedGraphCuts, chen2023ParallelNonNegativeMatrix, bouchareb2019ModelBasedCoclustering}. By addressing the shortcomings of traditional clustering methods, co-clustering not only enhances accuracy in pattern detection but also broadens the analytical scope, allowing for more sophisticated and context-aware data analysis.

Despite the advancements offered by co-clustering algorithms, they still have several limitations. 1) High Computational Complexity: The computational complexity can be prohibitively high, leading to prolonged processing times. This arises from the need to simultaneously analyze relationships across multiple dimensions, which grows exponentially with data size.
2) Significant Communication Overhead: Even if methods like partitioning are employed to handle large-scale data, the communication overhead remains substantial due to multiple rounds of data exchange. This is because co-clustering algorithms typically involve iterative optimization, requiring communication in each iteration, thus increasing the overall communication overhead.
3) Requirement for Sparse Matrices: Co-clustering algorithms typically require sparse matrices as input, and existing approaches struggle to adapt to dense matrices. This is because the algorithms often rely on the presence of missing values or zero values in the data matrix. Dense matrices may lead to difficulties in effectively handling a large number of non-zero elements.



To address the inherent challenges of traditional co-clustering algorithms, our research introduces an innovative hierarchical, agglomerative co-clustering approach. First,  we propose an optimal big matrix partitioning algorithm. This algorithm uses xxx (how the method works) to determine the optimal number and order of sub-matrices to be partitioned. This strategic partitioning allows for parallel processing of co-clustering tasks, significantly reducing the processing time and lessening the computational and storage demands on each processing unit.  
Second, we propose an efficient hierarchical ensemble method. This method combines results from sub-matrix co-clusterings, leveraging xxxx to enhance the accuracy and reliability of the clustering outcomes. Our hierarchical ensemble approach ensures a more robust and consistent clustering performance, addressing the issue of xxx. 
These two methodologies-partitioning and ensemble-synergistically work to enable a more scalable, efficient, and precise co-clustering process, particularly in handling large-scale, complex datasets.

The contributions of this paper are summarized as follows:
\begin{enumerate}
    \item \textbf{Hierarchical, Agglomerative Co-clustering Approach: }We present a novel hierarchical, agglomerative co-clustering approach to address the inherent challenges of traditional co-clustering algorithms. We propose an optimal big matrix partitioning algorithm that strategically determines the optimal number and order of sub-matrices to be partitioned, allowing for parallel processing of co-clustering tasks. This significantly reduces processing time and lessens computational and storage demands on each processing unit.
    \item \textbf{Efficient Hierarchical Ensemble Method} We propose an efficient hierarchical ensemble method that combines results from sub-matrix co-clusterings to enhance the accuracy and reliability of clustering outcomes. This approach ensures a more robust and consistent clustering performance, particularly addressing the issue of variability in co-clustering results.
    \item \textbf{Scalable and Precise Co-clustering with Verified Efficiency: } Our approach, rigorously evaluated across various scenarios, robustly demonstrates efficient co-clustering for large-scale, complex datasets. Experimental results show our method achieves state-of-the-art clustering effectiveness while speeding up processing by 30\%.
\end{enumerate}

The structure of this paper is as follows: Section 2 reviews related works; Section 3 describes our novel co-clustering approach; Section 4 explores the implementation and optimization of our probabilistic framework; Section 5 presents experiments and results, validating the efficacy of our approach; and Section 6 concludes the paper, summarizing our findings and discussing potential avenues for future research in this domain.
 