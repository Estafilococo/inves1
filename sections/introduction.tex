% sectionis/introduction.tex


\section{Introduction}
% Artificial Intelligence (AI) is an intense focus technology that reveals the most rapidly growing process on key technology and has a tremendous impact on an extensive range of domains from the health sector, as it supports complex data analysis, recognition of the patterns, and the processes leading to decision-making. Clustering is the execution of one of the significant roles within AI and involves a number of unsupervised learning techniques in the support of data categorization and pointing out patterns. Clustering algorithms, such as $k$-means\cite{lloyd1982LeastSquaresQuantization, macqueen1967MethodsClassificationAnalysis} and Gaussian Mixture Model (GMM)\cite{dempster1977MaximumLikelihoodIncomplete}, serve to aggregate data points based on shared attributes, thereby simplifying and interpreting the underlying data structure. However, these clustering algorithms often grapple with limitations, particularly when handling large, high-dimensional datasets. Their tendency to consider all features of data objects uniformly often leads to oversimplified interpretations and overlooks more nuanced, context-specific relationships within the data\cite{chen2023FastFlexibleBipartite, zhao2023MultiviewCoclusteringMultisimilarity, kumar2023CoclusteringBasedMethods}.
Artificial Intelligence (AI) is a rapidly advancing technology facilitating complex data analysis, pattern recognition, and decision-making processes. Within AI, clustering is part of several unsupervised learning techniques to categorize data and identify patterns, such as $k$-means \cite{lloyd1982LeastSquaresQuantization, macqueen1967MethodsClassificationAnalysis} and Gaussian Mixture Model (GMM) \cite{dempster1977MaximumLikelihoodIncomplete}. They group data points based on shared attributes, simplifying the interpretation of the underlying data structure. However, these algorithms tend to treat all features of data objects uniformly, leading to oversimplified interpretations and overlooking nuanced, context-specific relationships within the data, especially when dealing with large, high-dimensional datasets \cite{chen2023FastFlexibleBipartite, zhao2023MultiviewCoclusteringMultisimilarity, kumar2023CoclusteringBasedMethods}. Traditional clustering methods \cite{zhang2023AdaptiveGraphConvolution, yuan2023JointNetworkTopology, wu2023EffectiveClusteringStructured} only cluster rows (samples) or columns (features) in isolation.

% \textit{Co-clustering}\cite{cheng2000BiclusteringExpressionData, kluger2003SpectralBiclusteringMicroarray, yan2017CoclusteringMultidimensionalBig} is an approach that addresses these shortcomings by simultaneously grouping both rows (objects) and columns (features), revealing intricate correlations between two distinct data types, unlike other traditional cluster methods\cite{zhang2023AdaptiveGraphConvolution, yuan2023JointNetworkTopology, wu2023EffectiveClusteringStructured} that cluster either rows (objects) or columns (features) in isolation. This approach is particularly transformative in scenarios where the relationships between rows and columns are as crucial as the individual entities themselves. For example, in bioinformatics, co-clustering helps in identifying gene patterns by analyzing genes and conditions concurrently, thereby revealing more comprehensive biological insights\cite{higham2007SpectralClusteringIts, kluger2003SpectralBiclusteringMicroarray, madeira2004BiclusteringAlgorithmsBiological, zhao2012BiclusteringAnalysisPattern, golchev2015BiclusteringAnalysisGene}. Similarly, in text mining and recommender systems, it enables the discovery of latent relationships between users and items or documents and terms\cite{busygin2008BiclusteringDataMining, dhillon2001CoclusteringDocumentsWords, dhillon2007WeightedGraphCuts, chen2023ParallelNonNegativeMatrix, bouchareb2019ModelBasedCoclustering}. By addressing the shortcomings of traditional clustering methods, co-clustering not only enhances accuracy in pattern detection but also broadens the analytical scope, allowing for more sophisticated and context-aware data analysis.
\textit{Co-clustering}\cite{cheng2000BiclusteringExpressionData, kluger2003SpectralBiclusteringMicroarray, yan2017CoclusteringMultidimensionalBig} is an approach that groups rows (samples) and columns (features) simultaneously, to attempt to reveal complex correlations between two different data types. It is transformative in scenarios where the relationships between rows and columns are as important as the individual entities themselves. For example, in bioinformatics, co-clustering could identify gene related patterns leading to biological insights by analyzing genes and conditions concurrently \cite{higham2007SpectralClusteringIts, kluger2003SpectralBiclusteringMicroarray, madeira2004BiclusteringAlgorithmsBiological, zhao2012BiclusteringAnalysisPattern, golchev2015BiclusteringAnalysisGene}. In text mining and recommender systems, the discovery of latent relationships between users and items or documents and terms is possible \cite{busygin2008BiclusteringDataMining, dhillon2001CoclusteringDocumentsWords, dhillon2007WeightedGraphCuts, chen2023ParallelNonNegativeMatrix, bouchareb2019ModelBasedCoclustering}. Co-clustering extends traditional clustering methods, enhancing accuracy in pattern detection and broadening the scope of analyses.

However, the practical use of co-clustering methods can be limited, especially with big data due to:

\begin{enumerate}
    \item High Computational Complexity: This can be excessive due to the simultaneous analysis of relationships across multiple dimensions and may grow exponentially as data size increases.
    \item Significant Communication Overhead: Even when methods such as partitioning are used to handle large-scale data, the communication overhead remains significant due to multiple rounds of data exchange. This is because co-clustering algorithms usually involve iterative optimization, necessitating communication in each iteration, thereby increasing the overall communication overhead.
    \item Requirement for Sparse Matrices: Co-clustering algorithms usually require sparse matrices as input. Existing approaches have difficulty adapting to dense matrices because the algorithms rely on the presence of missing or zero values in the data matrix. Dense matrices can make it challenging to effectively handle a large number of non-zero elements.
\end{enumerate}

To address the inherent challenges of traditional co-clustering methods, our research introduces a novel hierarchical ensemble co-clustering method. First,  we propose an optimal large matrix partitioning algorithm, which uses the proposed probabilistic model to determine the optimal number and order of sub-matrices. This strategy enables simultaneous processing of co-clustering tasks, leading to a significant reduction in processing time and decreased computational and storage requirements for each processing unit.
Second, we propose an efficient ensemble method. This method combines results from sub-matrix co-clusterings, leveraging ensemble learning techniques to enhance the accuracy and reliability of the clustering outcomes. Our hierarchical ensemble approach ensures a more robust and consistent clustering performance, addressing the issue of heterogeneity and model uncertainty.
These two methodologies-partitioning and ensemble-synergistically work to enable a more scalable, efficient, and precise co-clustering process, particularly in handling large-scale, complex datasets.

The contributions of this paper are summarized as follows:
% \begin{enumerate}
%     \item \textbf{Hierarchical and Agglomerative Co-clustering Approach:} We present a novel hierarchical and agglomerative co-clustering approach to address the inherent challenges of traditional co-clustering algorithms. We propose an optimal big matrix partitioning algorithm that strategically determines the optimal number and order of sub-matrices, allowing for parallel processing of co-clustering tasks. This significantly reduces processing time and lessens computational and storage demands on each processing unit.
%     \item \textbf{Efficient Ensemble Method:} We propose an efficient ensemble method that combines results from sub-matrix co-clusterings to enhance the accuracy and reliability of clustering outcomes. This approach ensures a more robust and consistent clustering performance, particularly addressing the issue of variability in co-clustering results.
%     \item \textbf{Scalable and Precise Co-clustering with Verified Efficiency:} Our method has been validated critically through a wide spectrum of situations that have been undergone to be highly robust and a useful tool in co-clustering large, complex datasets. Our experimental results show a complete paradigm shift in the clustering effectiveness compared to techniques based on conventional methods. 
%     % Not surprisingly, in such a case, the remarkably reduced running time in our approach is 1/6 for a dense matrix and in the case of a sparse matrix, the reduction is up to the level of 70\%. 
%     We reduced the running time significantly to 1/6 in the scenario of a dense matrix and up to 70\% in the case of a sparse matrix.
%     This is an undeniably handsome improvement by the method, which argues for the very high competence of our technique in both types of matrices, but it makes a point of importance regarding benchmarks for redefinition clustering technology in the world of data analysis. 
% \end{enumerate}

\begin{enumerate}
    \item \textbf{Dynamic Partitioning Strategy:}
          We introduce a novel dynamic partitioning strategy on co-clustering big matrices.
          Combining the idea of divide-and-conquer with probabilistic modeling, our approach partitions large matrices into smaller sub-matrices, enabling parallel processing and reducing computational complexity.
          %   Proposing a probabilistic framework for dynamic matrix partitioning, we strike a balance between computational efficiency and annihilating the risk of missing co-clusters.
          We propose a probabilistic framework for dynamic matrix partitioning, striking a balance between computational efficiency and ensuring the detection of all relevant co-clusters.
    \item \textbf{Hierarchical Ensemble Co-Clustering:}
          We propose a hierarchical ensemble co-clustering method that combines results from individual sub-matrices.
          This ensemble ensures the completion of the co-clustering process with a pre-fixed number of iterations, enhancing the detection of stable co-clusters.
          The ensemble method significantly improves the robustness and reliability of the co-clustering process, addressing the issue of model uncertainty.
          Also, the modular design of our approach allows for easy integration with existing co-clustering models, making it adaptable across various domains.
    \item \textbf{Experiment Validation:}
          Our method has been validated critically through a wide spectrum of situations that have been undergone to be highly robust and a useful tool in co-clustering large, complex datasets.
          Our experimental results show a complete paradigm shift in the clustering effectiveness compared to techniques based on conventional methods.
          We reduced the running time significantly to 1/6 in the scenario of a dense matrix and up to 70\% in the case of a sparse matrix.
          This is an undeniably handsome improvement by the method, which argues for the very high competence of our technique in both types of matrices, but it makes a point of importance regarding benchmarks for redefinition clustering technology in the world of data analysis.
\end{enumerate}

The rest of this paper is organized as follows: Section \ref{sec:related_work} reviews related works; Section \ref{sec:method} describes our novel co-clustering approach; Section \ref{sec:experiment} presents experiments and results, validating the efficacy of our approach; and Section \ref{sec:conclude} concludes the paper, summarizing our findings and discussing potential avenues for future research in this domain.
