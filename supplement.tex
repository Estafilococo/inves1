%%%
 % File: ./supplement.tex
 % Created Date: Tuesday, February 11th 2025
 % Author: Zihan
 % -----
 % Last Modified: Tuesday, 11th February 2025 12:07:52 pm
 % Modified By: the developer formerly known as Zihan at <wzh4464@gmail.com>
 % -----
 % HISTORY:
 % Date      		By   	Comments
 % ----------		------	---------------------------------------------------------
%%%

\documentclass[onecolumn]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{enumitem}

% 保持与主文档相同的定理环境定义
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

\begin{document}
\title{Supplementary Material for: \\ DiMergeCo: A Scalable Framework for Large-Scale Co-Clustering with Theoretical Guarantees}

\author{}
\date{}

\maketitle

\section{Additional Theoretical Analysis}
\label{sec:appendix_theoretical_analysis}

This appendix provides a detailed account of the theoretical underpinnings of our proposed co-clustering framework. We present extended error bounds (\Cref{subsec:extended_error_bounds}), complexity analyses (\Cref{subsec:complexity_analysis}), and optimality conditions (\Cref{subsec:optimality_conditions}). We also include further technical lemmas and probability analyses (\Cref{subsec:probability_analysis}), along with detailed proofs (\Cref{subsec:detailed_proofs}).

\subsection{Extended Error Bounds}
\label{subsec:extended_error_bounds}

This section expands on the main text by giving precise statements and proofs of the error bounds that guarantee the quality of our divide-and-conquer approach.

\begin{theorem}[Global Error Bound]
    \label{thm:global_error_bound_appendix}
    Let \(\{\mathbf{B}_{(i,j)}\}\) be the submatrices after partitioning \(\mathbf{A}\), and let \(\hat{\mathbf{A}}\) be the reconstructed approximation from merging locally co-clustered results. If each block approximation error is bounded by \(\epsilon_{(i,j)}\), then
    \begin{equation}
        \|\mathbf{A} - \hat{\mathbf{A}}\|_F \;\le\; \sum_{i,j}\epsilon_{(i,j)}.
    \end{equation}
\end{theorem}

\begin{corollary}[Probabilistic Guarantee]
    \label{cor:prob_guarantee}
    If submatrices and the number of sampling/partitioning attempts \(T_p\) are chosen via the probabilistic model, for any prescribed \(\delta\),
    \begin{equation}
        P\bigl(\|\mathbf{A} - \hat{\mathbf{A}}\|_F \;\le\; \delta\bigr) \;\ge\; 1 - \alpha,
    \end{equation}
    where \(\alpha\) can be made arbitrarily small by increasing \(T_p\) or refining the partition.
\end{corollary}

\begin{theorem}[Co-clustering Recovery Error]
    \label{thm:recovery_error_appendix}
    For each submatrix, if the local solution is \(\epsilon_{(i,j)}\)-optimal and the approximation error of that block is \(\delta_{(i,j)}\), then
    \begin{equation}
        \|\mathbf{F}' - \mathbf{F}^*\|_F \;\le\; \sum_{i,j}\epsilon_{(i,j)} \;+\; g\bigl(\{\delta_{(i,j)}\}\bigr),
    \end{equation}
    where \(\mathbf{F}^*\) is the global optimal solution and \(g\) captures how local approximation errors propagate globally.
\end{theorem}

The above results imply that if each subproblem is well-approximated and locally optimized, the merged global solution remains close to a global optimum.

\subsection{Complexity Analysis}
\label{subsec:complexity_analysis}

We now analyze the computational complexity and communication overhead of our framework.

\begin{lemma}[Computational Complexity]
    \label{lem:computational_complexity}
    Assume the original matrix is \(m \times n\), partitioned into \(p \times q\) blocks, each of size at most \(\phi_{\max} \times \psi_{\max}\). Running a local co-clustering algorithm of complexity \(O\bigl(f(\phi_{\max}, \psi_{\max})\bigr)\) on each block yields a total time complexity of
    \begin{equation}
        O\Bigl(\max_{(i,j)} f(\phi_i,\psi_j)\Bigr),
    \end{equation}
    assuming a balanced partition among processing nodes.
\end{lemma}

\begin{theorem}[Communication Complexity]
    \label{thm:communication_overhead}
    The communication overhead is bounded by \(O(K \log K)\), where \(K\) is the number of local co-clusters. This arises because only cluster assignments and summary statistics are exchanged during merging, rather than broadcasting large-scale intermediate factorizations.
\end{theorem}

The above results show that each node handles only a fraction of the data, thus drastically reducing both the per-node time and memory requirements.

\subsection{Optimality Conditions}
\label{subsec:optimality_conditions}

Although global optimality cannot be guaranteed for this non-convex co-clustering problem, the following conditions improve solution quality and increase the likelihood of approaching near-optimal solutions.

\begin{lemma}[Partition Refinement]
    \label{lem:partition_refinement}
    Refining the partition (i.e., increasing \(p\) and \(q\) so that submatrices become smaller) improves the local approximation by reducing the maximum block size \(\phi_{\max}\times\psi_{\max}\). If each block error is bounded by \(\delta_{(i,j)}\), then
    \begin{equation}
        \epsilon_{\text{local}} \;\le\; C \,\max_{i,j}\{\phi_i\psi_j\},
    \end{equation}
    where \(C\) is a constant depending on data distributions.
\end{lemma}

\begin{theorem}[Sampling Convergence]
    \label{thm:sampling_convergence_appendix}
    Increasing the number of random partitions \(T_p\) exponentially decreases the probability of missing a co-cluster. Specifically,
    \begin{equation}
        P(\text{miss any co-cluster} ) \;\le\; K\,\exp(-\gamma T_p),
    \end{equation}
    where \(K\) is the number of significant co-clusters and \(\gamma>0\) depends on partition parameters.
\end{theorem}

\begin{proposition}[Near-Optimality]
    \label{prop:near_optimality}
    The framework can be made arbitrarily close to a global optimum by increasing the partition granularity (making submatrices smaller), repeating the partitioning process more often (\(T_p\)), and using stronger local algorithms. Concretely, there exist \(\phi_i, \psi_j, T_p\) such that
    \begin{equation}
        \|\mathbf{F}' - \mathbf{F}^*\|_F \;\; \text{can be made arbitrarily small.}
    \end{equation}
\end{proposition}

\subsection{Probability Analysis and Additional Lemmas}
\label{subsec:probability_analysis}

The probability analysis clarifies how large co-clusters remain intact within at least one submatrix. Below, we restate crucial lemmas and assumptions used in our proofs.

\noindent\textbf{Notations.}
We assume \(\mathbf{A}\in \mathbb{R}^{M\times N}\) is partitioned into \(m\times n\) blocks of size \(\phi_i\times\psi_j\), forming block set \(\{\mathbf{B}_{(i,j)}\}\). Each co-cluster \(\mathbf{C}_k\) spans rows \(\{I_k\}\) and columns \(\{J_k\}\). Thresholds \(T_m\) and \(T_n\) denote minimum row and column sizes of interest, respectively.

\begin{lemma}[Joint Probability of Co-Cluster Size]
    \label{lemma:joint_prob_size}
    For a co-cluster \(\mathbf{C}_k\) in block \(\mathbf{B}_{(i,j)}\), the probability that \(\mathbf{C}_k\) is smaller than \(T_m\) in rows and \(T_n\) in columns within \(\mathbf{B}_{(i,j)}\) is bounded by
    \begin{equation}
        P\bigl(M_{(i,j)}^{(k)} < T_m,\; N_{(i,j)}^{(k)} < T_n \bigr)\;\;\le\;\; \exp\bigl\lbrack -2(\phi_i\,m\,(s^{(k)})^2\;+\;\psi_j\,n\,(t^{(k)})^2)\bigr\rbrack ,
    \end{equation}
    where \(s^{(k)}\) and \(t^{(k)}\) denote normalized cluster dimensions.
\end{lemma}

\begin{assumption}[Gaussian Noise Model]
    \label{assump:gaussian_noise}
    A noise matrix \(\mathbf{N}\) with mean \(0\) and variance \(\sigma^2\) is added to the hidden co-cluster matrix \(\mathbf{A}\), forming the observed \(\mathbf{B}=\mathbf{A}+\mathbf{N}\). Each \(n_{ij}\sim \mathcal{N}(0,\sigma^2)\) i.i.d. If \(\max(\|\mathbf{B}\|_1,\|\mathbf{B}^T\|_1)/\sigma^2\geq \lambda\), certain matrix concentration inequalities apply.
\end{assumption}

\begin{definition}[Co-Cluster Score Function]
    \label{def:cc_score}
    For a submatrix \(\mathbf{A}_{I,J}\subseteq\mathbf{A}\), define
    \begin{equation}
        S_{row}(\mathbf{A}_{I,J}),\quad S_{col}(\mathbf{A}_{I,J}),\quad \text{and } S(\mathbf{A}_{I,J})
    \end{equation}
    exactly as in Definition 4 of the main text. These metrics measure homogeneity by row and column blocks, respectively.
\end{definition}

\begin{theorem}[Expected Co-Cluster Score]
    \label{thm:expected_cc_score}
    With noise \(\mathbf{N}\) as in~\Cref{assump:gaussian_noise}, one can bound the expected row and column scores of submatrix \(\mathbf{B}_{I,J}\) and their variances. For instance,
    \begin{equation}
        \mathbb{E}\bigl\lbrack S_{row}(I,J)\bigr\rbrack \;\ge\; |I||J|\Bigl(1\;-\;\exp\bigl\lbrack -2/\bigl(\alpha\,\max(\|\mathbf{B}\|_1,\|\mathbf{B}^T\|_1)\bigr\rbrack \Bigr),
    \end{equation}
    and similar bounds hold for \(\text{Var}\lbrack S_{row}(I,J)\rbrack \) and \(S_{col}(I,J)\).
\end{theorem}

\begin{theorem}[Chernoff Bound on Score Fluctuations]
    \label{thm:chernoff_score}
    Using Chernoff-type arguments, the probability that the submatrix co-cluster score \(S(I,J)\) deviates from its expectation by more than \(\epsilon\) is exponentially small in \(\epsilon^2\) and proportional to \(|I|\cdot|J|\). Formally,
    \begin{equation}
        P\Bigl(|S(I,J)-\mathbb{E}\lbrack S(I,J)\rbrack | > \epsilon\Bigr) \;\le\; \exp\Bigl(-\frac{\epsilon^2}{2|I||J|\sigma^2 \exp(-2/(\alpha\,\lambda))}\Bigr).
    \end{equation}
\end{theorem}

\subsection{Detailed Proofs}
\label{subsec:detailed_proofs}

We now present selected technical proofs referenced in the main text and earlier sections of this appendix.

% \subsubsection{Proof of Lemma \ref{lem:computational_complexity}}
\begin{IEEEproof}[Proof of Lemma \ref{lem:computational_complexity}]
    Assume an algorithm $\mathcal{L}$ for local co-clustering has worst-case complexity $O\bigl(f(\phi_{\max},\psi_{\max})\bigr)$, where $\phi_{\max}$ and $\psi_{\max}$ are maximum block dimensions. Since the matrix is partitioned into at most $p\times q$ blocks, each block is processed independently. Hence, the total computational cost is bounded by $p\times q \times O\bigl(f(\phi_{\max},\psi_{\max})\bigr)$ if done sequentially, or $O\bigl(\max f(\phi_i,\psi_j)\bigr)$ if done fully in parallel. Balancing the partition ensures a similar scale of submatrix for each node, yielding the stated complexity.
\end{IEEEproof}

% \subsubsection{Proof of Theorem \ref{thm:communication_overhead}}
\begin{IEEEproof}[Proof of Theorem \ref{thm:communication_overhead}]
    Communication cost arises primarily from merging partial co-clusters. If $K$ is the total number of partial co-clusters across blocks, each merge step involves comparing pairs or updating metadata in a priority queue. The maximum number of merges is $K-1$. Each merge can be handled in $O(\log K)$ if using a balanced tree or heap structure. Hence, $O(K\log K)$ is the dominating cost. We do not transmit large factorization data or entire blocks, only summary statistics, which remains $O(1)$ or $O(\log K)$ per merge. Thus, the overall communication overhead is $O(K\log K)$.
\end{IEEEproof}

% \subsubsection{Proof of Lemma \ref{lemma:joint_prob_size}}
\begin{IEEEproof}[Proof of Lemma \ref{lemma:joint_prob_size}]
    We define $\phi_i,\psi_j$ as the block dimensions. By modeling the row and column assignments as Bernoulli trials and applying standard Chernoff/Hoefding bounds, we get the probability that a co-cluster $\mathbf{C}_k$ remains wholly or partially in $\mathbf{B}_{(i,j)}$. The result follows by bounding the overlap probabilities and simplifying via the exponential expression in the lemma statement.
\end{IEEEproof}

\noindent \textbf{(Further proofs and additional lemmas can follow similarly.)}

\vspace{1em}
\noindent
\textbf{Summary of Appendix.} We have provided an extended theoretical account of our co-clustering framework, covering error bounds, complexity analyses, probability arguments, and detailed proofs. These results ensure that our method (1) preserves significant co-clusters with high probability, (2) scales favorably in both memory and computation, (3) converges to a stable local optimum, and (4) allows tuning of parameters to approach near-optimal solutions for large-scale data.

\begin{theorem}
    \label{thm:probability_co_cluster_detection_fixed}
    If the matrix $\mathbf{A}$ is partitioned into $m \times n$ blocks, each with fixed sizes $\phi_i \times \psi_j$, and the probability of failing to detect co-cluster $\mathbf{C}_k$ in any block is $P(\omega_k)$, then
    \begin{equation}
        P(\omega_k) \le \exp \left\{ -2 \left\lbrack \phi m \left(s^{(k)}\right)^2 + \psi n \left(t^{(k)}\right)^2 \right\rbrack \right\}
    \end{equation}
    Given $T_p$ independent random partitioning attempts, the probability of detecting the co-cluster $C_k$ at least once is
    \begin{equation}
        P \ge 1 - \exp \left\{ -2 T_p \lbrack \phi m (s^{(k)})^2 + \psi n (t^{(k)})^2\rbrack  \right\}
    \end{equation}
\end{theorem}

\begin{IEEEproof}
    Consider co-cluster $C_k$ with dimensions $M^{(k)} \times N^{(k)}$ falling into a block $B_{(i,j)}$ of size $\phi_i \times \psi_j$. The probability of co-cluster $C_k$ being fragmented (i.e., not entirely contained within any single block) in a single partitioning attempt is denoted by $P(\omega_k)$.

    The preservation probability for co-cluster $C_k$ in a specific block $B_{(i,j)}$ is determined by the likelihood that all rows and columns of $C_k$ are captured within that block. Using Chernoff-type bounds, we derive the upper bound for the failure probability as follows:
    \begin{equation}
        P(\omega_k) \le \exp \left\{ -2 \left\lbrack \phi m \left(s^{(k)}\right)^2 + \psi n \left(t^{(k)}\right)^2 \right\rbrack \right\}
    \end{equation}
    where:
    \begin{align*}
        s^{(k)} & = \frac{M^{(k)}}{M} - \frac{T_m - 1}{\phi_i}, \\
        t^{(k)} & = \frac{N^{(k)}}{N} - \frac{T_n - 1}{\psi_j}.
    \end{align*}

    Here, \(s^{(k)}\) and \(t^{(k)}\) represent the normalized minimum row and column sizes required for co-cluster \(C_k\) to be preserved within a block.

    Considering $T_p$ independent random partitioning attempts, the probability that co-cluster $C_k$ fails to be detected in all attempts is \( P(\omega_k)^{T_p} \). Consequently, the probability of successfully detecting co-cluster $C_k$ in at least one of the $T_p$ attempts is:
    \begin{equation}
        P \ge 1 - \exp \left\{ -2 T_p \left\lbrack \phi m \left(s^{(k)}\right)^2 + \psi n \left(t^{(k)}\right)^2 \right\rbrack \right\}
    \end{equation}

    This inequality ensures that by selecting appropriate values for \(m, n, \phi_i, \psi_j,\) and \(T_p\), the probability \(P\) of detecting co-cluster \(C_k\) meets or exceeds a desired threshold \(P_{\text{thresh}}\).
\end{IEEEproof}

\end{document}